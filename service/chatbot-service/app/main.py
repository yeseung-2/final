"""
Chatbot Service - LangChain í†µí•© ì„œë¹„ìŠ¤
"""
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import logging
import os
from dotenv import load_dotenv
import uvicorn

# LangChain imports
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

# ë¡œê±° ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("chatbot-service")

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

app = FastAPI(
    title="Chatbot Service",
    description="LangChain ê¸°ë°˜ ì±—ë´‡ ì„œë¹„ìŠ¤",
    version="1.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "https://eripotter.com",
        "https://www.eripotter.com",
        "http://localhost:3000",
        "http://localhost:8080"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# LangChain ëª¨ë¸ ì´ˆê¸°í™”
try:
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        logger.warning("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì‘ë‹µì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        llm = None
        embeddings = None
    else:
        llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.7,
            api_key=openai_api_key
        )
        embeddings = OpenAIEmbeddings(
            api_key=openai_api_key
        )
except Exception as e:
    logger.error(f"LangChain ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
    llm = None
    embeddings = None

# Pydantic ëª¨ë¸ë“¤
class ChatRequest(BaseModel):
    message: str
    user_id: Optional[str] = None
    company_id: Optional[str] = None
    context: Optional[str] = None

class ChatResponse(BaseModel):
    response: str
    sources: Optional[List[str]] = None
    confidence: Optional[float] = None

class DocumentUploadRequest(BaseModel):
    content: str
    metadata: Optional[Dict[str, Any]] = None
    company_id: Optional[str] = None

class DocumentResponse(BaseModel):
    document_id: str
    message: str

# ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
DEFAULT_PROMPT = ChatPromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ê¸°ì—…ì„ ìœ„í•œ ì „ë¬¸ì ì¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ 'ì• ë¦¬'ì…ë‹ˆë‹¤.
    
    ì‚¬ìš©ì ì§ˆë¬¸: {question}
    
    ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¼ ë‹µë³€í•´ì£¼ì„¸ìš”:
    1. ì „ë¬¸ì ì´ê³  ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”
    2. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”
    3. í•„ìš”ì‹œ êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ ë“¤ì–´ ì„¤ëª…í•˜ì„¸ìš”
    4. ê¸°ì—… í™˜ê²½ì— ì í•©í•œ ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”
    
    ë‹µë³€:"""
)

# ê¸°ë³¸ ì²´ì¸
if llm:
    basic_chain = DEFAULT_PROMPT | llm | StrOutputParser()
else:
    basic_chain = None

@app.get("/health")
async def health_check():
    """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    return {"status": "healthy", "service": "chatbot"}

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {"message": "Chatbot Service is running"}

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """ê¸°ë³¸ ì±„íŒ… ê¸°ëŠ¥"""
    try:
        logger.info(f"Chat request from user: {request.user_id}")
        
        if not basic_chain:
            # OpenAI API í‚¤ê°€ ì—†ì„ ë•Œ ê¸°ë³¸ ì‘ë‹µ
            return ChatResponse(
                response="ì•ˆë…•í•˜ì„¸ìš”! í˜„ì¬ AI ì„œë¹„ìŠ¤ê°€ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                confidence=0.5
            )
        
        # ê¸°ë³¸ ì²´ì¸ ì‹¤í–‰
        response = basic_chain.invoke({"question": request.message})
        
        return ChatResponse(
            response=response,
            confidence=0.8
        )
        
    except Exception as e:
        logger.error(f"Chat error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.post("/chat/contextual", response_model=ChatResponse)
async def contextual_chat(request: ChatRequest):
    """ì»¨í…ìŠ¤íŠ¸ë¥¼ ê³ ë ¤í•œ ì±„íŒ…"""
    try:
        logger.info(f"Contextual chat request from user: {request.user_id}")
        
        if not llm:
            return ChatResponse(
                response="ì•ˆë…•í•˜ì„¸ìš”! í˜„ì¬ AI ì„œë¹„ìŠ¤ê°€ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                confidence=0.5
            )
        
        # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš° í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
        if request.context:
            contextual_prompt = ChatPromptTemplate.from_template(
                """ë‹¹ì‹ ì€ ê¸°ì—…ì„ ìœ„í•œ ì „ë¬¸ì ì¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
                
                ì»¨í…ìŠ¤íŠ¸ ì •ë³´: {context}
                
                ì‚¬ìš©ì ì§ˆë¬¸: {question}
                
                ìœ„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”. í•œêµ­ì–´ë¡œ ì „ë¬¸ì ì´ê³  ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
                
                ë‹µë³€:"""
            )
            chain = contextual_prompt | llm | StrOutputParser()
            response = chain.invoke({
                "context": request.context,
                "question": request.message
            })
        else:
            response = basic_chain.invoke({"question": request.message})
        
        return ChatResponse(
            response=response,
            confidence=0.85
        )
        
    except Exception as e:
        logger.error(f"Contextual chat error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì»¨í…ìŠ¤íŠ¸ ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.post("/documents/upload", response_model=DocumentResponse)
async def upload_document(request: DocumentUploadRequest):
    """ë¬¸ì„œ ì—…ë¡œë“œ ë° ë²¡í„°í™”"""
    try:
        logger.info(f"Document upload request for company: {request.company_id}")
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        
        documents = text_splitter.split_text(request.content)
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        docs = [
            Document(
                page_content=doc,
                metadata=request.metadata or {}
            ) for doc in documents
        ]
        
        # ë²¡í„° ìŠ¤í† ì–´ ìƒì„± (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì˜êµ¬ ì €ì¥ì†Œ ì‚¬ìš©)
        vectorstore = Chroma.from_documents(
            documents=docs,
            embedding=embeddings
        )
        
        return DocumentResponse(
            document_id=f"doc_{len(docs)}",
            message=f"{len(docs)}ê°œì˜ ë¬¸ì„œ ì²­í¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤."
        )
        
    except Exception as e:
        logger.error(f"Document upload error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë¬¸ì„œ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.post("/chat/rag", response_model=ChatResponse)
async def rag_chat(request: ChatRequest):
    """RAG (Retrieval-Augmented Generation) ì±„íŒ…"""
    try:
        logger.info(f"RAG chat request from user: {request.user_id}")
        
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‚¬ìš©ì/íšŒì‚¬ë³„ ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰
        # ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ ì‘ë‹µìœ¼ë¡œ ëŒ€ì²´
        
        rag_prompt = ChatPromptTemplate.from_template(
            """ë‹¹ì‹ ì€ ê¸°ì—… ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
            
            ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ:
            {context}
            
            ì‚¬ìš©ì ì§ˆë¬¸: {question}
            
            ìœ„ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
            ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì„¸ìš”.
            
            ë‹µë³€:"""
        )
        
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ë¥¼ contextì— ë„£ì–´ì•¼ í•¨
        mock_context = "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤."
        
        chain = rag_prompt | llm | StrOutputParser()
        response = chain.invoke({
            "context": mock_context,
            "question": request.message
        })
        
        return ChatResponse(
            response=response,
            sources=["ë¬¸ì„œ ê²€ìƒ‰ ê¸°ëŠ¥ì€ ê°œë°œ ì¤‘ì…ë‹ˆë‹¤"],
            confidence=0.7
        )
        
    except Exception as e:
        logger.error(f"RAG chat error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"RAG ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.middleware("http")
async def log_requests(request: Request, call_next):
    """ìš”ì²­ ë¡œê¹… ë¯¸ë“¤ì›¨ì–´"""
    logger.info(f"ğŸ“¥ ìš”ì²­: {request.method} {request.url.path}")
    response = await call_next(request)
    logger.info(f"ğŸ“¤ ì‘ë‹µ: {response.status_code}")
    return response

if __name__ == "__main__":
    port = int(os.getenv("PORT", 8003))
    logger.info(f"ğŸ¤– ì±—ë´‡ ì„œë¹„ìŠ¤ ì‹œì‘ - í¬íŠ¸: {port}")
    uvicorn.run(
        "app.main:app",
        host="0.0.0.0",
        port=port,
        log_level="info"
    )
